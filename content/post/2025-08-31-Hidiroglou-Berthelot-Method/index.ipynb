{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "date: 2025-08-31\n",
    "title: \"Anomaly Detection: Hidiroglou-Berthelot or HB-edit\"\n",
    "draft: false\n",
    "toc: false \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.asasrms.org/Proceedings/y2023/files/HB_JSM_2023.pdf\n",
    "\n",
    "https://ssc.ca/sites/default/files/survey/documents/SSC2003_R_Belcher.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "The Hidiroglou‑Berthelot method, or HB‑edit, was introduced by Hidiroglou and Berthelot in 1986 to enhance outlier detection in periodic business surveys, particularly where units (e.g. companies, survey respondents) exhibit wide variations in size. Detecting outliers in survey data can be difficult due to the extreme variation in the size of respondents.\n",
    "\n",
    "\n",
    "1. For each entity $i$, compute the **ratio** of its current value $x_i(t)$ to its previous value $x_i(t-1)$:  \n",
    "\n",
    "   $$r_i = \\frac{x_i(t)}{x_i(t-1)}$$\n",
    "\n",
    "2. Center these ratios around their **median** $r_{Q_2}$ or $r_{M}$ through a transformation generating $s_i$, which is symmetric around zero:  \n",
    "\n",
    "   $$\n",
    "   s_i =\n",
    "   \\begin{cases}\n",
    "   1 - \\frac{r_{Q_2}}{r_i}, & \\text{if } 0 < r_i < r_{Q_2}, \\\\\n",
    "   \\frac{r_i}{r_{Q_2}} - 1, & \\text{if } r_i \\geq r_{Q_2}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "\n",
    "Then, to account for the size of the observation the HB method creates an effector vector, $e_k$, by scaling the symmetric ratios as followis:\n",
    "\n",
    "3. Incorporate the **size** of the unit—by taking the maximum of $x_i(t)$ and $x_i(t-1)$, raised to the power of a tuning parameter $U$ (between 0 and 1)—to compute the **effect score**:  \n",
    "\n",
    "   $$E_i = s_i \\times \\bigl[\\max(x_i(t), x_i(t-1))\\bigr]^U, \\text{ where } 0 \\le u \\le 1$$\n",
    "\n",
    "   - Larger units require smaller relative changes to be flagged as outliers.  \n",
    "   - Smaller units tolerate proportionally larger fluctuations.  \n",
    "\n",
    "4. Define outlier boundaries based on percentiles or quartiles of the $E_i$ distribution. Typically:  \n",
    "\n",
    "   $$[E_M - C \\times d_{Q1},\\; E_M + C \\times d_{Q3}]$$\n",
    "\n",
    "   where:  \n",
    "   - $E_M$ is the median of $E_i$;  \n",
    "   - $d_{Q1} = \\max(E_M - E_{Q1}, |A \\times E_M|)$;  \n",
    "   - $d_{Q3} = \\max(E_{Q3} - E_M, |A \\times E_M|)$;  \n",
    "   - $A$ is a small constant (commonly 0.05);  \n",
    "   - $C$ scales how wide these bounds are (commonly 4–7).  \n",
    "\n",
    "Units whose $E_i$ fall outside this interval are flagged as outliers.\n",
    "\n",
    "## Why Is HB-edit Useful?\n",
    "\n",
    "- **Size-aware flexibility**: By incorporating unit size via $U$, the method adjusts tolerance for change.  \n",
    "- **Symmetric detection**: Captures both unusually large and unusually small changes.  \n",
    "- **Data-driven, nonparametric**: No strong distributional assumptions.  \n",
    "- **Adjustable sensitivity**: Parameters $U$, $A$, and $C$ allow analysts to tune sensitivity.\n",
    "\n",
    "## Assumptions & Practical Considerations\n",
    "\n",
    "**Key assumptions and caveats:**\n",
    "\n",
    "- Ratio-of-change distribution should be smooth and roughly symmetric.  \n",
    "- Parameter tuning requires care—defaults are often $U = 0.4$, $A = 0.05$, $C = 4$–7.  \n",
    "- Many identical ratios can cause quartile issues—percentiles (e.g. 10th & 90th) may work better.  \n",
    "- HB-edit is **univariate**; multivariate anomaly detection requires different methods.\n",
    "\n",
    "**Practical workflow:**\n",
    "\n",
    "1. Plot the distribution of $E_i$ scores.  \n",
    "2. Experiment with parameter values.  \n",
    "3. Use adjusted boxplots or other robust diagnostics.  \n",
    "4. Always review flagged outliers in context.\n",
    "\n",
    "## Summary Table: HB-edit Snapshot\n",
    "\n",
    "| Element              | Description |\n",
    "|----------------------|-------------|\n",
    "| **Ratio $r_i$**        | Change between periods |\n",
    "| **Centered $s_i$**     | Symmetric score around median |\n",
    "| **Effect $E_i$**       | Size-weighted score |\n",
    "| **Parameters**         | $U, A, C$ for tuning |\n",
    "| **Bounds**             | Median-based, robust intervals |\n",
    "| **Use Cases**          | Surveys, census, business data |\n",
    "| **Strengths**          | Size-aware, symmetric, flexible |\n",
    "| **Limitations**        | Needs tuning, univariate only |\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "The **Hidiroglou-Berthelot (HB-edit) method** is a robust and interpretable tool for outlier detection—especially well-suited for longitudinal survey or administrative data where units vary widely in size. With careful parameter tuning and visualization, HB-edit highlights meaningful anomalies without overwhelming analysts with false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "To demonstrate this method I am going to use the 2020 and 2010 Census tract-level population estimates. Code to create this dataset from the Census API is available here:\n",
    "\n",
    "<details>\n",
    "<summary>Click to view code</summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "state_fips = [f\"{i:02d}\" for i in range(1, 57) if i not in {3, 7, 14, 43, 52}]\n",
    "\n",
    "def fetch_tracts(year, var, base):\n",
    "    \"\"\"\n",
    "    Fetch tract-level totals for all states for a given year.\n",
    "    year: 2010 or 2020 (only used for clarity)\n",
    "    var:  'P001001' (2010) or 'P1_001N' (2020)\n",
    "    base: 'https://api.census.gov/data/2010/dec/sf1' or 'https://api.census.gov/data/2020/dec/pl'\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    states = [f\"{i:02d}\" for i in range(1, 57) if i not in {3, 7, 14, 43, 52}]\n",
    "\n",
    "    for st in tqdm(states, desc=f\"Downloading {year} tracts\"):\n",
    "        # Example:\n",
    "        # .../data/2020/dec/pl?get=NAME,P1_001N&for=tract:*&in=state:01&in=county:*\n",
    "        url = f\"{base}?get=NAME,{var}&for=tract:*&in=state:{st}&in=county:*\"\n",
    "        response = requests.get(url, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data[1:], columns=data[0])\n",
    "        # Ensure numeric population\n",
    "        df[var] = pd.to_numeric(df[var], errors=\"coerce\")\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"NAME\", var, \"state\", \"county\", \"tract\"])\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "# -------- fetch --------\n",
    "# 2010 Decennial SF1, total population P001001\n",
    "df10_raw = fetch_tracts(\n",
    "    year=2010,\n",
    "    var=\"P001001\",\n",
    "    base=\"https://api.census.gov/data/2010/dec/sf1\"\n",
    ")\n",
    "\n",
    "# 2020 PL 94-171, total population P1_001N\n",
    "df20_raw = fetch_tracts(\n",
    "    year=2020,\n",
    "    var=\"P1_001N\",\n",
    "    base=\"https://api.census.gov/data/2020/dec/pl\"\n",
    ")\n",
    "\n",
    "# -------- tidy + merge --------\n",
    "df10 = df10_raw.rename(columns={\"P001001\": \"POP_2010\"})\n",
    "df20 = df20_raw.rename(columns={\"P1_001N\": \"POP_2020\"})\n",
    "\n",
    "# Keep consistent keys\n",
    "keep_cols = [\"NAME\", \"state\", \"county\", \"tract\"]\n",
    "df10 = df10[keep_cols + [\"POP_2010\"]]\n",
    "df20 = df20[keep_cols + [\"POP_2020\"]]\n",
    "\n",
    "# Merge on tract FIPS (state+county+tract) and NAME\n",
    "tracts = df10.merge(df20, on=[\"state\", \"county\", \"tract\", \"NAME\"], how=\"outer\")\n",
    "\n",
    "# Optional: build a full 11-digit tract GEOID (2 state + 3 county + 6 tract)\n",
    "tracts[\"GEOID\"] = tracts[\"state\"].str.zfill(2) + tracts[\"county\"].str.zfill(3) + tracts[\"tract\"].str.zfill(6)\n",
    "\n",
    "# Reorder columns nicely\n",
    "tracts = tracts[[\"GEOID\", \"NAME\", \"state\", \"county\", \"tract\", \"POP_2010\", \"POP_2020\"]]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hidiroglou_berthelot_outliers(y_k, x_k, u = 0.5, a = 0.05, c = 4, quantile_lo = 0.25, quantile_hi = 0.75, verbose = False):\n",
    "    \"\"\"\n",
    "    Hidiroglou-Berthelot Method (...) for Outliers.\n",
    "\n",
    "    Assume numerator and denominator are same length.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_k : 1D data\n",
    "        Data to test.\n",
    "    x_k : 1D data\n",
    "        Data to test.\n",
    "    u : float\n",
    "        Parameter. Controls curve of final boundaries. Commonly (u = 0.50)\n",
    "    a : float\n",
    "        Parameter. Ensures upper and ower bounds are not arbitrarily close to the median. (a = 0.05)\n",
    "    c : float\n",
    "        Parameter. Controls the width of the acceptance region. (c = 4)\n",
    "    quantlie_lo : float\n",
    "        Parameter. Optional quantile for lower bound of effects vector. Usually 25th percentile, but could be 10th.\n",
    "    quantlie_lo : float\n",
    "        Parameter. Optional quantile for lower bound of effects vector. Usually 75th percentile, but could be 90th.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outliers : ndarray of bool, shape (n,)\n",
    "        Boolean mask indicating which points in `data` are considered outliers. \n",
    "        True for detected outliers, False otherwise (including NaNs).\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Hidiroglou, M.A., and Berthelot, J.-M. (1986). ”Statistical Editing and Imputation for Periodic Business Surveys”. Survey Methodology, 12, 73-83.\n",
    "    \"\"\"\n",
    "    y_k = np.array(y_k)\n",
    "    x_k = np.array(x_k)\n",
    "\n",
    "    # Check length\n",
    "    if y_k.shape[0] != x_k.shape[0]:\n",
    "        raise ValueError(f\"y_k and x_k must be the same length. Got lengths: {len(y_k)} and {len(x_k)}\")\n",
    "\n",
    "    # Ignore NaNs and zeros\n",
    "    valid = (x_k != 0) & (y_k != 0) & ~np.isnan(x_k) & ~np.isnan(y_k)\n",
    "\n",
    "    # Ratio\n",
    "    r_k = y_k[valid] / x_k[valid]\n",
    "\n",
    "    # Ratio Median\n",
    "    r_Q2 = np.quantile(r_k, 0.50)\n",
    "\n",
    "    # Centering transformation\n",
    "    s_k = np.where(\n",
    "        (r_k < r_Q2) & (r_k > 0),\n",
    "        1 - (r_Q2 / r_k), # 0 < r_k < r_Q2\n",
    "        (r_k / r_Q2) - 1  # Otherwise\n",
    "    )\n",
    "\n",
    "    # Effects vector\n",
    "    e_k = s_k * np.maximum(x_k[valid], y_k[valid])**u\n",
    "    \n",
    "    e_Q1 = np.quantile(e_k, quantile_lo) \n",
    "    e_Q2 = np.quantile(e_k, 0.50) \n",
    "    e_Q3 = np.quantile(e_k, quantile_hi) \n",
    "\n",
    "    # Upper and Lower HB Bounds\n",
    "    bound_lo = e_Q2 - c * max(e_Q2 - e_Q1, a * np.abs(e_Q2))\n",
    "    bound_hi = e_Q2 + c * max(e_Q3 - e_Q2, a * np.abs(e_Q2))\n",
    "\n",
    "    # Masks effects vectors as outliers\n",
    "    outlier_effects = (e_k < bound_lo) | (e_k > bound_hi)\n",
    "\n",
    "    # Creates mask like original length of data\n",
    "    outliers = np.full_like(x_k, False, dtype = bool)\n",
    "    outliers[valid] = outlier_effects\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from IPython.display import Image, display\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"census_tract_population_2010_2020.csv\").dropna()\n",
    "df = df[(df[\"POP_2010\"] > 0) & (df[\"POP_2020\"] > 0)]\n",
    "\n",
    "df[\"hb_edit_outliers\"] = hidiroglou_berthelot_outliers(\n",
    "    df[\"POP_2020\"],\n",
    "    df[\"POP_2010\"],\n",
    "    u = 0.5, a = 0.05, c = 10, quantile_lo = 0.1, quantile_hi = 0.9\n",
    ")\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x = \"POP_2010\",\n",
    "    y = \"POP_2020\",\n",
    "    color = \"hb_edit_outliers\",\n",
    "    title = \"Census Tract Population - 2020 versus 2010\"\n",
    ")\n",
    "fig.write_image('fig1.png')\n",
    "#display(Image(filename=\"fig1.png\"))\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Hidiroglou, M.A., and Berthelot, J.-M. (1986). ”Statistical Editing and Imputation for Periodic Business Surveys”. Survey Methodology, 12, 73-83."
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "summary": "Easily blog from Jupyter notebooks!"
  },
  "kernelspec": {
   "display_name": "PyML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
