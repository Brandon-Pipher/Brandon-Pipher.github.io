{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5bac615e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "date: 2025-08-01\n",
    "title: \"RAG: Retrieval Augmented Generation Agent - Demo\"\n",
    "draft: true\n",
    "toc: false \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cf2ac",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines a large language model with an external knowledge sourceâ€”like a database, document library, or web searchâ€”to produce more accurate, context-aware answers.\n",
    "\n",
    "A business-related example would be providing an LLM with proprietary information such as technical specificiations, user manuals, or product specifications in order to better assist customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3582166",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "I chose to use **llama3** for the backend large language model and **mxbai-embed-large** as the embedding model. The models are running through **Ollama** using the following Nvidia GPU and associated drivers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f2c13",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal gpu details</summary>\n",
    "\n",
    "``` bash\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3080        On  |   00000000:0C:00.0  On |                  N/A |\n",
    "|  0%   27C    P5             32W /  340W |     689MiB /  10240MiB |     31%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6dd3f2",
   "metadata": {},
   "source": [
    "Their respective models and their sizes are also shown below:\n",
    "\n",
    "``` bash\n",
    "NAME                        ID              SIZE      PROCESSOR    UNTIL              \n",
    "mxbai-embed-large:latest    468836162de7    1.2 GB    100% GPU     4 minutes from now    \n",
    "llama3:latest               365c0bd3c000    6.7 GB    100% GPU     4 minutes from now  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eaaaf4",
   "metadata": {},
   "source": [
    "Embedding models are models that are trained specifically to generate vector embeddings. Vector embeddings are just arrays of numbers that represent semantic meaning for a given sequence of text.\n",
    "\n",
    "<img src=\"what-are-embeddings.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a439a1",
   "metadata": {},
   "source": [
    "# Streamlit\n",
    "\n",
    "Getting started with Ollama is a one-liner thanks to their install script. \n",
    "\n",
    "Once installed it is just a matter of running `ollama pull <model>` and then `ollama run <model>`. It starts a local server by default at `http://localhost:11434` which is what you use to interact with the models.\n",
    "\n",
    "The code below demonstrates a quick mock-up of a ChatGPT setup using streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3496f6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<details>\n",
    "<summary>Click to reveal ChatGPT-like clone using Ollama and Streamlit</summary>\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "\n",
    "st.title(\"ðŸ’¬ ChatGPT-like Demo (Ollama + Streamlit)\")\n",
    "\n",
    "if \"model\" not in st.session_state:\n",
    "    st.session_state.model = \"llama3\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display existing chat\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "# Handle user input\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response_text = \"\"\n",
    "        payload = {\n",
    "            \"model\": st.session_state.model,\n",
    "            \"messages\": st.session_state.messages,\n",
    "            \"stream\": True\n",
    "        }\n",
    "\n",
    "        response = requests.post(OLLAMA_URL, json=payload, stream=True)\n",
    "\n",
    "        placeholder = st.empty()\n",
    "\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                data = json.loads(line.decode(\"utf-8\"))\n",
    "                content = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                response_text += content\n",
    "                placeholder.markdown(response_text)\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92dbd2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<img src=\"demo.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5bbf3",
   "metadata": {},
   "source": [
    "For storing data I opted to use ChromaDB to create a vectorstore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
